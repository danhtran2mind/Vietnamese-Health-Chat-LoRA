{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-10T07:13:44.224257Z",
     "iopub.status.busy": "2025-07-10T07:13:44.223963Z",
     "iopub.status.idle": "2025-07-10T07:17:43.679900Z",
     "shell.execute_reply": "2025-07-10T07:17:43.678225Z",
     "shell.execute_reply.started": "2025-07-10T07:13:44.224231Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m167.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m4.9/6.3 MB\u001b[0m \u001b[31m149.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q unsloth==2025.6.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:17:43.683375Z",
     "iopub.status.busy": "2025-07-10T07:17:43.682917Z",
     "iopub.status.idle": "2025-07-10T07:17:43.693423Z",
     "shell.execute_reply": "2025-07-10T07:17:43.692398Z",
     "shell.execute_reply.started": "2025-07-10T07:17:43.683312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:17:43.694980Z",
     "iopub.status.busy": "2025-07-10T07:17:43.694632Z",
     "iopub.status.idle": "2025-07-10T07:17:45.933178Z",
     "shell.execute_reply": "2025-07-10T07:17:45.930906Z",
     "shell.execute_reply.started": "2025-07-10T07:17:43.694950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/3228981502.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munsloth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munsloth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Reduce VRAM usage by reducing fragmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "import datasets\n",
    "import numpy as np\n",
    "import unsloth\n",
    "import torch\n",
    "import trl \n",
    "import transformers\n",
    "\n",
    "\n",
    "print(\"datasets.__version__\", datasets.__version__)\n",
    "print(\"numpy.__version__\", np.__version__)\n",
    "print(\"unsloth.__version__\", unsloth.__version__)\n",
    "print(\"torch.__version__\", torch.__version__)\n",
    "print(\"transformers.__version__\", transformers.__version__)\n",
    "print(\"trl.__version__\", trl.__version__)\n",
    "\n",
    "# datasets.__version__ 3.6.0\n",
    "# numpy.__version__ 1.26.4\n",
    "# unsloth.__version__ 2025.6.3\n",
    "# torch.__version__ 2.7.0+cu126\n",
    "# transformers.__version__ 4.51.3\n",
    "# trl.__version__ 0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.933777Z",
     "iopub.status.idle": "2025-07-10T07:17:45.934027Z",
     "shell.execute_reply": "2025-07-10T07:17:45.933925Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.933913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# To temporary Model hub\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import login\n",
    "# Initialize API\n",
    "login(\"<your_huggingface_token>\")\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.935639Z",
     "iopub.status.idle": "2025-07-10T07:17:45.935971Z",
     "shell.execute_reply": "2025-07-10T07:17:45.935853Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.935836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "api = HfApi()\n",
    "\n",
    "repo_id = \"danhtran2mind/Qwen-3-0.6B-Instruct-Vi-Medical-LoRA\"\n",
    "save_path = \"Qwen-3-0.6B-Instruct-Vi-Medical-LoRA\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "snapshot_download(repo_id=repo_id, repo_type=\"model\", local_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.937313Z",
     "iopub.status.idle": "2025-07-10T07:17:45.937712Z",
     "shell.execute_reply": "2025-07-10T07:17:45.937551Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.937533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth.chat_templates import train_on_responses_only  \n",
    "import torch\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.938854Z",
     "iopub.status.idle": "2025-07-10T07:17:45.939139Z",
     "shell.execute_reply": "2025-07-10T07:17:45.939037Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.939025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_id = \"tmnam20/ViMedAQA\"\n",
    "subset = \"disease\"\n",
    "dataset = load_dataset(dataset_id, subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.940245Z",
     "iopub.status.idle": "2025-07-10T07:17:45.940596Z",
     "shell.execute_reply": "2025-07-10T07:17:45.940485Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.940467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge training and validation samples\n",
    "test_samples = concatenate_datasets([dataset['validation'],\n",
    "                                         dataset['test']])\n",
    "\n",
    "# Shuffle the merged dataset with a fixed random seed\n",
    "training_samples = dataset['train']\n",
    "test_samples = test_samples.shuffle(seed=42)\n",
    "\n",
    "print(\"Number of training samples:\", training_samples.num_rows)\n",
    "print(\"Number of test samples:\", test_samples.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T06:33:20.174144Z",
     "iopub.status.busy": "2025-07-09T06:33:20.173882Z",
     "iopub.status.idle": "2025-07-09T06:33:20.195436Z",
     "shell.execute_reply": "2025-07-09T06:33:20.194728Z",
     "shell.execute_reply.started": "2025-07-09T06:33:20.174126Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_idx': 'disease_13345',\n",
       " 'question': 'Khi soi bằng đèn thường, triệu chứng điển hình của viêm mống mắt là gì?',\n",
       " 'answer': 'Đồng tử 2 bên không đều nhau, đồng tử bên bệnh nhỏ hơn bên bình thường.',\n",
       " 'context': '5.1 Nhìn về mặt đại thể (khi soi bằng đèn thường) - Đồng tử 2 bên không đều nhau (bên bệnh nhỏ hơn) Có thể phát hiện đồng tử co bên bệnh lý co nhiều hơn bên bình thường, điều này có thể do viêm nhiều dẫn đến dính thành sau mống mắt lại với nhau và với thuỷ tinh thể. Thông thường kích thường đồng tử là 2 – 3 mm, đều 2 bên. - Mủ ở đáy mống mắt (mủ tiền phòng) Đây là hiện tượng các tế bào viêm cùng với chất tiết (chứa nhiều fibrin). Mủ nằm ở dưới là do nguyên nhân trọng lực. 5.2 Khám mắt qua đèn khe: (split lamp) Đây là công cụ chuyên dụng của bác sĩ chuyên khoa Mắt. Qua đèn khe, bác sĩ có thể phát hiện: - Các tế bào nằm giữa giác mạc và mống mắt Số lượng tế bào này trên diện tích 1 mm² giúp phân độ nặng của viêm màng bồ đào. - Độ mờ thuỷ dịch Như đã miêu tả, thuỷ dịch là dịch trong suốt nằm giữa giác mạc ở ngoài cùng và mống mắt ở trong. Khi viêm màng bồ đào trước, dịch này sẽ bị vẩn đục. Tương tự như việc đếm số lượng tế bào ở thuỷ dịch, ta cũng phân độ nhờ vào mức độ đục của nó. - Sự lắng đọng keratic Sự lắng đọng keratic tạo thành tam giác Arlt. Tam giác Arlt được tạo thành nhờ các kết tụ tế bào viêm nằm ngay sau giác mạc. Trên đèn khe tam giác này có đỉnh nằm trên, đáy nằm dưới. Tạo hình này do trọng lực gây ra. - Xơ hoá fibrin Do mủ trong thuỷ dịch + viêm màng bồ đào trước diễn ra lâu dài, không điều trị kịp thời. - Ngoài ra còn nhiều dấu hiệu khác trên đèn khe Nốt mống mắt, dính mống mắt – thuỷ tinh thế, thiểu sản mống mắt, tăng sinh mạch máu mống mắt.',\n",
       " 'title': 'Triệu chứng - Triệu chứng qua thăm khám',\n",
       " 'keyword': 'Viêm mống mắt',\n",
       " 'topic': 1,\n",
       " 'article_url': 'https://youmed.vn/tin-tuc/viem-mong-mat-nguyen-nhan-gay-mu-loa/',\n",
       " 'author': 'Bác sĩ Nguyễn Đoàn Trọng Nhân',\n",
       " 'author_url': 'https://youmed.vn/tin-tuc/bac-si/nguyen-doan-trong-nhan/'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.942141Z",
     "iopub.status.idle": "2025-07-10T07:17:45.942550Z",
     "shell.execute_reply": "2025-07-10T07:17:45.942359Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.942341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen3-0.6B\"\n",
    "# Load the model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_id,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.943482Z",
     "iopub.status.idle": "2025-07-10T07:17:45.943731Z",
     "shell.execute_reply": "2025-07-10T07:17:45.943632Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.943620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_squad_sample_to_conversation(sample):\n",
    "    # get the question and context for this sample\n",
    "    question = sample['question']\n",
    "    context = sample['context']\n",
    "\n",
    "    answers = sample['answer']\n",
    "    if len(answers) == 0 :\n",
    "      answer = \"The context does not provide an answer...\"\n",
    "    else:\n",
    "      answer = sample['answer']\n",
    "\n",
    "    # now we define an initial model prompt defining the task and giving the model the context passage\n",
    "    instruction_prompt_template = '''Bạn là một trợ lý hữu ích được giao nhiệm vụ trích xuất các đoạn văn trả lời câu hỏi của người dùng từ một ngữ cảnh cho trước. Xuất ra các đoạn văn chính xác từng từ một trả lời câu hỏi của người dùng. Không xuất ra bất kỳ văn bản nào khác ngoài các đoạn văn trong ngữ cảnh. Xuất ra lượng tối thiểu để trả lời câu hỏi, ví dụ chỉ 2-3 từ từ đoạn văn. Nếu không thể tìm thấy câu trả lời trong ngữ cảnh, xuất ra 'Ngữ cảnh không cung cấp câu trả lời...'\n",
    "\n",
    "    Ngữ cảnh: {context}'''\n",
    "\n",
    "    # now we'll convert these into a list of messages for our conversation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction_prompt_template.format(context=context)},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    \n",
    "    sample_conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return {\"text\": sample_conversation, \"messages\": messages, \"answer\": answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.948008Z",
     "iopub.status.idle": "2025-07-10T07:17:45.948312Z",
     "shell.execute_reply": "2025-07-10T07:17:45.948167Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.948155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "conversation_training_samples = training_samples.map(convert_squad_sample_to_conversation)\n",
    "conversation_test_samples = test_samples.map(convert_squad_sample_to_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T06:33:39.685184Z",
     "iopub.status.busy": "2025-07-09T06:33:39.684507Z",
     "iopub.status.idle": "2025-07-09T06:33:39.690094Z",
     "shell.execute_reply": "2025-07-09T06:33:39.689529Z",
     "shell.execute_reply.started": "2025-07-09T06:33:39.685163Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nếu bạn có các triệu chứng nghi ngờ của nhiễm nấm da chân, hãy liên hệ với bác sĩ ngay để được tham vấn điều trị sớm. Điều này sẽ giúp bạn kiểm soát bệnh tình và ngăn ngừa các biến chứng nghiêm trọng.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_test_samples[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T06:33:39.690951Z",
     "iopub.status.busy": "2025-07-09T06:33:39.690732Z",
     "iopub.status.idle": "2025-07-09T06:33:41.294255Z",
     "shell.execute_reply": "2025-07-09T06:33:41.293529Z",
     "shell.execute_reply.started": "2025-07-09T06:33:39.690935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"Bạn là một trợ lý hữu ích được giao nhiệm vụ trích xuất các đoạn văn trả lời câu hỏi của người dùng từ một ngữ cảnh cho trước. Xuất ra các đoạn văn chính xác từng từ một trả lời câu hỏi của người dùng. Không xuất ra bất kỳ văn bản nào khác ngoài các đoạn văn trong ngữ cảnh. Xuất ra lượng tối thiểu để trả lời câu hỏi, ví dụ chỉ 2-3 từ từ đoạn văn. Nếu không thể tìm thấy câu trả lời trong ngữ cảnh, xuất ra 'Ngữ cảnh không cung cấp câu trả lời...'\\n\\n    Ngữ cảnh: Những mẹo này có thể giúp bạn tránh bị nấm da chân hoặc giảm bớt các triệu chứng nếu nhiễm nấm xảy ra: - Giữ chân khô ráo, đặc biệt là giữa các ngón chân. Đi chân trần để chân thoát khí nhiều nhất có thể khi ở nhà. Lau khô các kẽ ngón chân sau khi tắm.\\n- Thay tất thường xuyên. Nếu chân bạn ra nhiều mồ hôi, hãy thay tất hai lần một ngày.\\n- Đi giày nhẹ, thông thoáng. Tránh giày làm bằng chất liệu tổng hợp, chẳng hạn như nhựa vinyl hoặc cao su.\\n- Mang thay đổi các đôi giày. Đừng mang cùng một đôi mỗi ngày để giày có thời gian khô sau mỗi lần sử dụng.\\n- Bảo vệ đôi chân của bạn ở những nơi công cộng. Mang dép hoặc giày không thấm nước xung quanh hồ bơi công cộng, phòng tắm vòi sen và phòng để tủ khóa.\\n- Điều trị bàn chân của bạn. Dùng bột, tốt nhất là thuốc chống nấm, bôi lên chân hàng ngày.\\n- Đừng dùng chung giày. Điều này sẽ làm nguy cơ lây nhiễm nấm. Như vậy, nấm da chân là bệnh lý khá dễ lây truyền khi sinh hoạt ở những nơi công cộng. Vì vậy qua bài viết này chúng tôi hi vọng các bạn đã có những kiến thức để phòng tránh chúng. Mặt khác, trong trường hợp xuất hiện những triệu chứng nghi ngờ của nhiễm nấm. Hãy liên hệ ngay với chuyên gia y khoa để được tham vấn điều trị sớm bạn nhé.\",\n",
       "  'role': 'system'},\n",
       " {'content': 'Khi nào người bệnh nên liên hệ với bác sĩ?', 'role': 'user'},\n",
       " {'content': 'Nếu bạn có các triệu chứng nghi ngờ của nhiễm nấm da chân, hãy liên hệ với bác sĩ ngay để được tham vấn điều trị sớm. Điều này sẽ giúp bạn kiểm soát bệnh tình và ngăn ngừa các biến chứng nghiêm trọng.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_test_samples[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T01:40:31.412482Z",
     "iopub.status.busy": "2025-07-10T01:40:31.412268Z",
     "iopub.status.idle": "2025-07-10T01:40:31.418173Z",
     "shell.execute_reply": "2025-07-10T01:40:31.417470Z",
     "shell.execute_reply.started": "2025-07-10T01:40:31.412465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nBạn là một trợ lý hữu ích được giao nhiệm vụ trích xuất các đoạn văn trả lời câu hỏi của người dùng từ một ngữ cảnh cho trước. Xuất ra các đoạn văn chính xác từng từ một trả lời câu hỏi của người dùng. Không xuất ra bất kỳ văn bản nào khác ngoài các đoạn văn trong ngữ cảnh. Xuất ra lượng tối thiểu để trả lời câu hỏi, ví dụ chỉ 2-3 từ từ đoạn văn. Nếu không thể tìm thấy câu trả lời trong ngữ cảnh, xuất ra 'Ngữ cảnh không cung cấp câu trả lời...'\\n\\n    Ngữ cảnh: Những mẹo này có thể giúp bạn tránh bị nấm da chân hoặc giảm bớt các triệu chứng nếu nhiễm nấm xảy ra: - Giữ chân khô ráo, đặc biệt là giữa các ngón chân. Đi chân trần để chân thoát khí nhiều nhất có thể khi ở nhà. Lau khô các kẽ ngón chân sau khi tắm.\\n- Thay tất thường xuyên. Nếu chân bạn ra nhiều mồ hôi, hãy thay tất hai lần một ngày.\\n- Đi giày nhẹ, thông thoáng. Tránh giày làm bằng chất liệu tổng hợp, chẳng hạn như nhựa vinyl hoặc cao su.\\n- Mang thay đổi các đôi giày. Đừng mang cùng một đôi mỗi ngày để giày có thời gian khô sau mỗi lần sử dụng.\\n- Bảo vệ đôi chân của bạn ở những nơi công cộng. Mang dép hoặc giày không thấm nước xung quanh hồ bơi công cộng, phòng tắm vòi sen và phòng để tủ khóa.\\n- Điều trị bàn chân của bạn. Dùng bột, tốt nhất là thuốc chống nấm, bôi lên chân hàng ngày.\\n- Đừng dùng chung giày. Điều này sẽ làm nguy cơ lây nhiễm nấm. Như vậy, nấm da chân là bệnh lý khá dễ lây truyền khi sinh hoạt ở những nơi công cộng. Vì vậy qua bài viết này chúng tôi hi vọng các bạn đã có những kiến thức để phòng tránh chúng. Mặt khác, trong trường hợp xuất hiện những triệu chứng nghi ngờ của nhiễm nấm. Hãy liên hệ ngay với chuyên gia y khoa để được tham vấn điều trị sớm bạn nhé.<|im_end|>\\n<|im_start|>user\\nKhi nào người bệnh nên liên hệ với bác sĩ?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nNếu bạn có các triệu chứng nghi ngờ của nhiễm nấm da chân, hãy liên hệ với bác sĩ ngay để được tham vấn điều trị sớm. Điều này sẽ giúp bạn kiểm soát bệnh tình và ngăn ngừa các biến chứng nghiêm trọng.<|im_end|>\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_test_samples[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.949117Z",
     "iopub.status.idle": "2025-07-10T07:17:45.949412Z",
     "shell.execute_reply": "2025-07-10T07:17:45.949261Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.949249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(  \n",
    "    model,  \n",
    "    r=16,  \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],  \n",
    "    lora_alpha=16,  \n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  \n",
    "    random_state=42,  \n",
    "    use_rslora=False,  \n",
    "    loftq_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.951861Z",
     "iopub.status.idle": "2025-07-10T07:17:45.952141Z",
     "shell.execute_reply": "2025-07-10T07:17:45.952012Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.952002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arguments = TrainingArguments(  \n",
    "                resume_from_checkpoint=\"./Qwen-3-0.6B-Instruct-Vi-Medical-LoRA\",\n",
    "                per_device_train_batch_size=8,  \n",
    "                per_device_eval_batch_size=8,  \n",
    "                gradient_accumulation_steps=4,  \n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=100,\n",
    "                logging_steps=50,\n",
    "                save_steps=100,\n",
    "                warmup_steps=30,\n",
    "                save_total_limit=4,\n",
    "                num_train_epochs=50, # 5\n",
    "                # max_steps=80,\n",
    "                save_strategy=\"steps\",\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                learning_rate=2e-4,  \n",
    "                fp16=not is_bfloat16_supported(),  \n",
    "                bf16=is_bfloat16_supported(),  \n",
    "                optim=\"adamw_8bit\",  \n",
    "                weight_decay=0.01,  \n",
    "                lr_scheduler_type=\"linear\",  \n",
    "                seed=42,  \n",
    "                output_dir=\"Qwen-3-0.6B-Instruct-Vi-Medical-LoRA\",  \n",
    "                report_to=\"none\",\n",
    "                load_best_model_at_end=True,  # Load weights with lowest val loss\n",
    "        \t\tgreater_is_better=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.953779Z",
     "iopub.status.idle": "2025-07-10T07:17:45.954080Z",
     "shell.execute_reply": "2025-07-10T07:17:45.953941Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.953925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "trainer = SFTTrainer(  \n",
    "    model=model,  \n",
    "    tokenizer=tokenizer,  \n",
    "    train_dataset=conversation_training_samples,  \n",
    "    eval_dataset=conversation_test_samples,  \n",
    "    # dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,  \n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  \n",
    "    dataset_num_proc=2,  \n",
    "    packing=False,  # Can make training 5x faster for short sequences.  \n",
    "    args=arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T01:40:50.075089Z",
     "iopub.status.busy": "2025-07-10T01:40:50.074845Z",
     "iopub.status.idle": "2025-07-10T01:40:50.266339Z",
     "shell.execute_reply": "2025-07-10T01:40:50.265607Z",
     "shell.execute_reply.started": "2025-07-10T01:40:50.075069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t\t  checkpoint-15800  checkpoint-19800  checkpoint-7938\n",
      "..\t\t  checkpoint-15900  checkpoint-19845  checkpoint-8600\n",
      ".cache\t\t  checkpoint-16000  checkpoint-20500  checkpoint-8700\n",
      "checkpoint-11800  checkpoint-16100  checkpoint-20600  checkpoint-8800\n",
      "checkpoint-11900  checkpoint-17000  checkpoint-20700  checkpoint-8820\n",
      "checkpoint-11907  checkpoint-17100  checkpoint-20727  checkpoint-9600\n",
      "checkpoint-13000  checkpoint-17200  checkpoint-6400   checkpoint-9700\n",
      "checkpoint-13100  checkpoint-17300  checkpoint-6500   checkpoint-9800\n",
      "checkpoint-13200  checkpoint-18100  checkpoint-6600   checkpoint-9900\n",
      "checkpoint-13230  checkpoint-18200  checkpoint-6615   .gitattributes\n",
      "checkpoint-14300  checkpoint-18300  checkpoint-700    README.md\n",
      "checkpoint-14400  checkpoint-18400  checkpoint-7700\n",
      "checkpoint-14500  checkpoint-19600  checkpoint-7800\n",
      "checkpoint-14553  checkpoint-19700  checkpoint-7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -a Qwen-3-0.6B-Instruct-Vi-Medical-LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 16100 \t0.263000 \t0.552778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T07:17:45.954945Z",
     "iopub.status.idle": "2025-07-10T07:17:45.955180Z",
     "shell.execute_reply": "2025-07-10T07:17:45.955072Z",
     "shell.execute_reply.started": "2025-07-10T07:17:45.955061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start train process\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T01:40:50.267678Z",
     "iopub.status.busy": "2025-07-10T01:40:50.267388Z",
     "iopub.status.idle": "2025-07-10T06:49:43.798258Z",
     "shell.execute_reply": "2025-07-10T06:49:43.797602Z",
     "shell.execute_reply.started": "2025-07-10T01:40:50.267652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 14,121 | Num Epochs = 50 | Total steps = 22,050\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 10,092,544/600,000,000 (1.68% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22050' max='22050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22050/22050 5:08:32, Epoch 49/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.415671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.415204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.414333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.412141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.412027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.410530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.409665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.407233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.405407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.406128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.404468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.139800</td>\n",
       "      <td>0.403904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.403558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "# Start train process\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:06:31.078185Z",
     "iopub.status.busy": "2025-07-10T07:06:31.077876Z",
     "iopub.status.idle": "2025-07-10T07:06:31.272786Z",
     "shell.execute_reply": "2025-07-10T07:06:31.271904Z",
     "shell.execute_reply.started": "2025-07-10T07:06:31.078164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!echo abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./Qwen-3-0.6B-Instruct-Vi-Medical-LoRA\")  # Local saving\n",
    "tokenizer.save_pretrained(\"./Qwen-3-0.6B-Instruct-Vi-Medical-LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:07:39.156202Z",
     "iopub.status.busy": "2025-07-10T07:07:39.155496Z",
     "iopub.status.idle": "2025-07-10T07:07:39.159520Z",
     "shell.execute_reply": "2025-07-10T07:07:39.158930Z",
     "shell.execute_reply.started": "2025-07-10T07:07:39.156182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = (\"Khi nghi ngờ bị loét dạ dày tá tràng nên đến khoa nào \"\n",
    "          \"tại bệnh viện để thăm khám?\")\n",
    "          \n",
    "# answer = (\"Ngủ ngáy gây gián đoạn hô hấp dẫn đến mất ngủ, mệt mỏi, ảnh hưởng đến \"\n",
    "#           \"học tập và làm việc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:09:13.729536Z",
     "iopub.status.busy": "2025-07-10T07:09:13.729040Z",
     "iopub.status.idle": "2025-07-10T07:09:15.645859Z",
     "shell.execute_reply": "2025-07-10T07:09:15.645303Z",
     "shell.execute_reply.started": "2025-07-10T07:09:13.729514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nếu nghi ngờ bị loét dạ dày tá tràng, bạn nên đến phòng khám chuyên khoa Nội khoa để được thăm khám và chẩn đoán chính xác.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 2048, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.9, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T17:25:32.408392Z",
     "iopub.status.busy": "2025-07-08T17:25:32.407620Z",
     "iopub.status.idle": "2025-07-08T17:25:34.540891Z",
     "shell.execute_reply": "2025-07-08T17:25:34.540163Z",
     "shell.execute_reply.started": "2025-07-08T17:25:32.408366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nếu nghi ngờ bị loét dạ dày tá tràng, bạn nên đến phòng khám chuyên khoa Nội thần học để được thăm khám và chẩn đoán chính xác.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
